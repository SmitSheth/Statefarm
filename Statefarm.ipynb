{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import math,os,json,sys\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "import numpy as np\n",
    "from numpy.random import random, randn\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential  \n",
    "from keras.layers.core import Flatten,Dropout,Dense\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from clr_callback import *\n",
    "\n",
    "from glob import glob\n",
    "import bcolz\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/kaggle/statefarm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=4, class_mode='categorical',\n",
    "                target_size=(224,224)):\n",
    "    return gen.flow_from_directory(dirname,target_size=target_size,batch_size=batch_size,class_mode=class_mode,shuffle=shuffle)\n",
    "\n",
    "def onehot(x):\n",
    "    return to_categorical(x.classes,num_classes=10)\n",
    "\n",
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "    \n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Make validation sets \n",
    "\n",
    "for item in os.listdir(path+\"/train\"):\n",
    "    path1=path+\"/train/\"+item\n",
    "    %cd $path1\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(200): os.rename(shuf[i],path+'/valid/' + item +'/'+ shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "# batches shuffle must be set to False when pre-computing features\n",
    "batches=get_batches(path +'/train',batch_size=batch_size,shuffle=False)\n",
    "valid_batches=get_batches(path +'/valid',batch_size=batch_size*2,shuffle=False)\n",
    "test_batches=get_batches(path +'/test',batch_size=32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential([\n",
    "#         BatchNormalization(axis=1, input_shape=(224,224,3)),\n",
    "#         Convolution2D(32,3,3, activation='relu'),\n",
    "#         BatchNormalization(axis=1),\n",
    "#         MaxPooling2D((3,3)),\n",
    "#         Convolution2D(64,3,3, activation='relu'),\n",
    "#         BatchNormalization(axis=1),\n",
    "#         MaxPooling2D((3,3)),\n",
    "#         Convolution2D(128,3,3, activation='relu'),\n",
    "#         BatchNormalization(axis=1),\n",
    "#         MaxPooling2D((3,3)),\n",
    "#         Flatten(),\n",
    "#         Dense(200, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(200, activation='relu'),\n",
    "#         BatchNormalization(),\n",
    "#         Dropout(0.5),\n",
    "#         Dense(10, activation='softmax')\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(Adam(lr=1e-5),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "# model.fit_generator(batches,batches.samples/batch_size,epochs=2,validation_data=valid_batches,\n",
    "#                     validation_steps=valid_batches.samples/valid_batches.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=VGG16(weights='imagenet',include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen_t=image.ImageDataGenerator(rotation_range=15,height_shift_range=0.05,shear_range=0.1,channel_shift_range=20,width_shift_range=0.1)\n",
    "# batches=get_batches(path +'/train',batch_size=batch_size)\n",
    "\n",
    "# model.layers.pop()\n",
    "# for layer in model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last = model.layers[-1].output\n",
    "# y = Dropout(0.3)(last)\n",
    "# x = Dense(10, activation=\"softmax\")(y)\n",
    "# model = Model(model.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers = model.layers\n",
    "# # Get the index of the first dense layer...\n",
    "# first_dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "# # ...and set this and all subsequent layers to trainable\n",
    "# for layer in layers[first_dense_idx:]: layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(Adam(lr=1e-5),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "# model.fit_generator(batches,batches.samples/batch_size,epochs=5,validation_data=valid_batches,\n",
    "#                     validation_steps=valid_batches.samples/valid_batches.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights(path+'/models/da_conv8_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_labels=onehot(batches)\n",
    "val_data_labels=onehot(valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VGG16(weights='imagenet',include_top=True)\n",
    "last_conv_idx = [i for i,l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-compute feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_train_feat=conv_model.predict_generator(batches, np.ceil(batches.samples/batches.batch_size))\n",
    "conv_val_feat = conv_model.predict_generator(valid_batches, np.ceil(valid_batches.samples/valid_batches.batch_size))\n",
    "conv_test_feat = conv_model.predict_generator(test_batches, np.ceil(test_batches.samples/test_batches.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'/results/conv_val_feat.dat', conv_val_feat)\n",
    "save_array(path+'/results/conv_test_feat.dat', conv_test_feat)\n",
    "save_array(path+'/results/conv_train_feat.dat', conv_train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_feat = load_array(path+'/results/conv_train_feat.dat')\n",
    "conv_val_feat = load_array(path+'/results/conv_val_feat.dat')\n",
    "conv_test_feat=load_array(path+'/results/conv_test_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Augmentation\n",
    "gen_t = image.ImageDataGenerator(rotation_range=15, height_shift_range=0.05, \n",
    "                shear_range=0.1, channel_shift_range=20, width_shift_range=0.1)\n",
    "aug_batches = get_batches(path+'/train', gen_t, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_conv_feat = conv_model.predict_generator(aug_batches, 5*(np.ceil(aug_batches.samples/aug_batches.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_array(path+'/results/da_conv_feat2.dat', aug_conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_conv_feat = load_array(path+'/results/da_conv_feat2.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate them with our training data\n",
    "aug_conv_feat = np.concatenate([aug_conv_feat, conv_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate them with our training labels\n",
    "aug_trn_labels = np.concatenate([train_data_labels]*6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CYCLIC LEARNING RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform LR test to find the optimal learning rate range\n",
    "clr = CyclicLR(base_lr=0.0001, max_lr=1, step_size=np.ceil(batches.samples/(batches.batch_size)), mode='triangular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(SGD(momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bn_model.fit(conv_feat, train_data_labels, batch_size=batch_size, epochs=1, \n",
    "             validation_data=(conv_val_feat, val_data_labels),callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = clr.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['lr'],history['acc'])\n",
    "# plt.plot(acc)  \n",
    "plt.title('Accuracy plot against accuracy')\n",
    "plt.xscale('log')\n",
    "plt.xticks\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.legend(['lr'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the graph, the model starts converging when lr=0.02, so it is reasonable to set base lr = 0.01. Furthermore, above a learning rate of 0.05 the accuracy rise gets rough and eventually\n",
    "begins to drop so it is reasonable to set max lr = 0.006."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['lr'],history['loss'])\n",
    "# plt.plot(acc)  \n",
    "plt.title('Learning rate plot against loss')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.legend(['lr'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clr = CyclicLR(base_lr=0.02, max_lr=0.05, step_size=np.ceil(aug_conv_feat.shape[0]/(batch_size)*2), mode='triangular') #for Vgg16\n",
    "clr = CyclicLR(base_lr=0.005, max_lr=0.02, step_size=np.ceil(aug_conv_feat.shape[0]/(batch_size)*2), mode='triangular')  #For Vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(SGD(lr=0.005), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.fit(aug_conv_feat, aug_trn_labels, batch_size=batch_size, epochs=5, \n",
    "             validation_data=(conv_val_feat, val_data_labels),callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'/models/conv8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.load_weights(path+'/models/conv8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSEUDO-LABELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=bn_model.predict(conv_test_feat,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_trn = 0\n",
    "i_test = 0\n",
    "\n",
    "# iterate through 800 mini-batch\n",
    "num_iter = int(np.ceil(aug_conv_feat.shape[0]/(batch_size))*5)\n",
    "\n",
    "size_trn=64\n",
    "size_test=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batch_per_epoch_trn = int(aug_conv_feat.shape[0]/size_trn)\n",
    "num_batch_per_epoch_test = int(conv_test_feat.shape[0]/size_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an shuffled index array\n",
    "index_trn = np.random.permutation(num_batch_per_epoch_trn)\n",
    "index_test = np.random.permutation(num_batch_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_iter):\n",
    "    # Get an index number from the shuffled index array for the current loop i\n",
    "    i_trn = index_trn[i%num_batch_per_epoch_trn]\n",
    "    i_test = index_test[i%num_batch_per_epoch_test] \n",
    "\n",
    "    comb_features = np.concatenate((aug_conv_feat[(size_trn*i_trn):size_trn*(i_trn+1)],\n",
    "                                   conv_test_feat[(size_test*i_test):size_test*(i_test+1)]),axis=0)\n",
    "    \n",
    "    comb_labels = np.concatenate((aug_trn_labels[(size_trn*i_trn):size_trn*(i_trn+1)],\n",
    "                                 result[(size_test*i_test):size_test*(i_test+1)]), axis=0)\n",
    "    \n",
    "    loss=bn_model.train_on_batch(comb_features, comb_labels)\n",
    "    print(loss)\n",
    "    # Shuffle index array after model had trained on the last mini-batch.\n",
    "    if (i+1)%num_batch_per_epoch_trn == 0:\n",
    "        index_trn = np.random.permutation(num_batch_per_epoch_trn)\n",
    "    if (i+1)%num_batch_per_epoch_test == 0:\n",
    "        index_test = np.random.permutation(num_batch_per_epoch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'/models/conv9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.load_weights(path+'/models/conv9.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_model.fit(aug_conv_feat, aug_trn_labels, batch_size=batch_size, epochs=1, \n",
    "             validation_data=(conv_val_feat, val_data_labels),callbacks=[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=bn_model.predict(conv_test_feat,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm = do_clip(result,0.99)\n",
    "subm_name = path+'/results/result.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "# submission = pd.DataFrame(result, columns=classes)\n",
    "submission.insert(0, 'img', [a[4:] for a in test_batches.filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
